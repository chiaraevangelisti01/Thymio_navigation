{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mobile robotics project - Group 29**\n",
    "\n",
    "##### **Group members**:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Brown Andrew Michael   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Elmaleh Daniel Abraham   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Evangelisti Chiara  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Ziegler Mathieu   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**\n",
    "- [1. Introduction](#Introduction)\n",
    "  - [1.1 Abstract](#Abstract)\n",
    "  - [1.2 Environment setup](#Environment-setup)\n",
    "- [2. Vision](#Vision)\n",
    "  - [2.1 ](#Section-2.1)\n",
    "- [3. Global Navigation](#Global-navigation)\n",
    "  - [3.1](#Section-3.1)\n",
    "- [4.Local Navigation](#Local-navigation)\n",
    "  - [4.1 ](#Section-4.1)\n",
    "- [5. Extended Kalman Filter](#Extended-Kalman-filter)\n",
    "  - [5.1](#Section-5.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as mt\n",
    "import cv2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _test=True\n",
    "else:_test=True\n",
    "\n",
    "\n",
    "\n",
    "_obs_min_area=250\n",
    "\n",
    "\n",
    "\n",
    "class Vision:\n",
    "    def __init__(self, camID, tagID, areaPX=(1080, 720), areaCM=(118, 90), calibration_filename=\"cal.npy\"):\n",
    "        self._camID=camID\n",
    "        self._thymio_tagID=tagID\n",
    "        self._cal_file=calibration_filename\n",
    "\n",
    "\n",
    "        #self._cap=cv2.VideoCapture('http://192.168.0.56:8080/video')\n",
    "        self._cap=cv2.VideoCapture(camID)\n",
    "        self._cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "        self._cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "        self._areaPX=areaPX\n",
    "        self._areaCM=areaCM\n",
    "\n",
    "\n",
    "        if _test:\n",
    "            cv2.namedWindow('Video Feed', cv2.WINDOW_NORMAL)\n",
    "            cv2.setMouseCallback('Video Feed',self._get_hsv_value)\n",
    "\n",
    "        \n",
    "        # Load the predefined dictionary of ArUco markers\n",
    "        arucoDict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_ARUCO_ORIGINAL)\n",
    "        arucoParams = cv2.aruco.DetectorParameters()\n",
    "\n",
    "        # Detect ArUco markers in the image\n",
    "        self._detector = cv2.aruco.ArucoDetector(arucoDict, arucoParams)\n",
    "        self._frame=None\n",
    "\n",
    "        self._hsv_margin=None\n",
    "        self._hsv_buffer_size=30\n",
    "        self._hsv_buffer = np.full((self._hsv_buffer_size, 3), np.nan)\n",
    "        try:\n",
    "            self._obstacles_hsv = np.load(self._cal_file)\n",
    "        except FileNotFoundError:\n",
    "            self._obstacles_hsv = None\n",
    "\n",
    "        self.obstacles=None\n",
    "        self.thymio_pos=None\n",
    "        self.target=np.array([0,0])\n",
    "        self.direction=None\n",
    "        self.usable_thymio_pos=False\n",
    "        self.usable_obstacles=False\n",
    "        \n",
    "    def get_thymio_pos(self):\n",
    "        if  not self.usable_thymio_pos: return None\n",
    "        pos = self.thymio_pos\n",
    "        factor=self._areaCM[0]/self._areaPX[0]\n",
    "        pos = [pos[0]*factor,pos[1]*factor,pos[2]]\n",
    "        return pos\n",
    "    \n",
    "    def get_obstacles(self):\n",
    "        if not self.usable_obstacles: return None\n",
    "        factor=self._areaCM[0]/self._areaPX[0]\n",
    "        transformed_polygons = []\n",
    "        for polygon in self.obstacles:\n",
    "            transformed_polygon = [([i[0][0] * factor,i[0][1] * (factor)]) for i in polygon]\n",
    "            transformed_polygons.append(transformed_polygon)\n",
    "        return transformed_polygons\n",
    "    \n",
    "    def get_target(self):\n",
    "        pos = self.target\n",
    "        factor=self._areaCM[0]/self._areaPX[0]\n",
    "        return [pos[0]*factor,pos[1]*factor]\n",
    "    \n",
    "    def update(self):\n",
    "        self.usable_thymio_pos=False\n",
    "        self.usable_obstacles=False\n",
    "\n",
    "        ret, self._frame = self._cap.read()\n",
    "        if not ret:\n",
    "            raise ValueError(\"Camera Missing\")\n",
    "\n",
    "        if self._warp():\n",
    "            self.usable_obstacles=self._update_obstacles()\n",
    "            self.usable_thymio_pos=self._update_thymio()\n",
    "            if _test:\n",
    "                cv2.circle(self._frame, (tuple(self.target[:2])), 5, [0,0,255],-1)\n",
    "                if self.usable_obstacles: self._draw_contours()\n",
    "                if self.usable_thymio_pos: self._draw_robot()\n",
    "\n",
    "        if _test: cv2.imshow('Video Feed', self._frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            pass\n",
    "\n",
    "        self.get_thymio_pos()\n",
    "\n",
    "    def _get_mask(self, image, colors):\n",
    "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        mask = cv2.inRange(hsv, colors[0], colors[1])\n",
    "        return mask\n",
    "\n",
    "    def _warp(self):\n",
    "        # Convert the frame to grayscale\n",
    "        #gray = cv2.cvtColor(self._frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        (corners, ids, rejected) = self._detector.detectMarkers(self._frame)\n",
    "\n",
    "        # Check if there are four markers detected\n",
    "\n",
    "        centers=[0,0,0,0]\n",
    "        # Assuming the ArUco markers are placed at the four corners of the desired area\n",
    "        # Sort the corners if necessary. The order should be: top-left, top-right, bottom-right, bottom-left\n",
    "        id_seen=[]\n",
    "        if ids is None: return False\n",
    "\n",
    "        for corner, id in zip(corners, ids):\n",
    "            if id[0] in [0,6,2,3]:\n",
    "            # Calculate the center for each marker\n",
    "                center = np.mean(corner[0], axis=0)\n",
    "                val=id[0]\n",
    "                if val==6: val=1\n",
    "                centers[val] = center\n",
    "                id_seen.append(id)\n",
    "            if id == 5:\n",
    "                center = np.mean(corner[0], axis=0)\n",
    "                pos = center+ corner[0,1]-corner[0,2]\n",
    "                self._update_hsv_obs(pos)\n",
    "        if sorted(id_seen) != [0,2,3,6]:\n",
    "            return False\n",
    "        # Flatten the corner points and convert to float32\n",
    "        pts1 = np.float32([c for c in centers])\n",
    "        # Define the points for the transformed image\n",
    "        _width=self._areaPX[0]\n",
    "        _height=self._areaPX[1]\n",
    "        pts2 = np.float32([[0, 0], [_width, 0], [_width, _height], [0, _height]])\n",
    "\n",
    "        # Compute the perspective transform matrix and apply it\n",
    "        matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "        self._frame = cv2.warpPerspective(self._frame, matrix, (_width, _height))\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def _update_obstacles(self):\n",
    "        self.obstacles=[]\n",
    "        if self._obstacles_hsv is None:\n",
    "            return False\n",
    "        obs_mask = self._get_mask(self._frame, self._obstacles_hsv)\n",
    "        temp_contours, dump = cv2.findContours(obs_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        obs=[]\n",
    "        for i in temp_contours:\n",
    "            if cv2.contourArea(i)> _obs_min_area:\n",
    "                obs.append(i)\n",
    "\n",
    "\n",
    "        epsilon_factor = 0.02  # This factor controls the approximation precision\n",
    "        for cnt in obs:\n",
    "            epsilon = epsilon_factor * cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            if len(approx) > 2:  # Filter out small shapes, adjust as needed\n",
    "                self.obstacles.append(approx)\n",
    "        return True\n",
    "\n",
    "    def _update_thymio(self):\n",
    "        gray = cv2.cvtColor(self._frame, cv2.COLOR_BGR2GRAY)\n",
    "        (corners, ids, rejected) = self._detector.detectMarkers(self._frame)\n",
    "        center=np.array([0,0])\n",
    "        time_seen=0\n",
    "        direction=None\n",
    "        if ids is None: return\n",
    "        for corner, id in zip(corners, ids):\n",
    "            if id==self._thymio_tagID:\n",
    "                center = np.mean(corner[0], axis=0)\n",
    "                time_seen+=1\n",
    "                direction=corner[0][0]-corner[0][1]\n",
    "            if id == 7:\n",
    "                self.target=np.mean(corner[0], axis=0).astype(np.int64)\n",
    "        if time_seen != 1: return False\n",
    "        self.direction=np.array([-direction[1], direction[0]])\n",
    "        angle=mt.atan2(*self.direction[::-1])\n",
    "        self.thymio_pos=[int(center[0]), int(center[1]), angle]\n",
    "\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _draw_contours(self):\n",
    "        for cnt in self.obstacles:\n",
    "            epsilon = 0.02 * cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            cv2.drawContours(self._frame, [approx], -1, (0, 255, 0), 3)\n",
    "\n",
    "    def _draw_robot(self):\n",
    "        cv2.circle(self._frame, (tuple(self.thymio_pos[:2])), 5, [0,0,255],-1)\n",
    "        start=tuple(self.thymio_pos[:2])\n",
    "        stop =tuple((self.thymio_pos[:2] + self.direction).astype(int))\n",
    "        cv2.line(self._frame,start,stop,(255,0,0),5)\n",
    "\n",
    "    def _get_hsv_value(self, event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            # Get the HSV value at the clicked point\n",
    "            hsv_frame = cv2.cvtColor(self._frame, cv2.COLOR_BGR2HSV)\n",
    "            hsv_value = hsv_frame[y, x]\n",
    "            print(\"HSV Value at ({}, {}): {}\".format(x, y, hsv_value))\n",
    "    \n",
    "    def _update_hsv_obs(self, pos):\n",
    "        hsv_frame = cv2.cvtColor(self._frame, cv2.COLOR_BGR2HSV)\n",
    "        pos=pos.astype(np.int64)\n",
    "        if pos[0]>self._areaPX[0]: return\n",
    "        if pos[1]>self._areaPX[1]: return\n",
    "        if _test: cv2.circle(self._frame,pos, 5, [0,255,0],-1 )\n",
    "        middle_point = hsv_frame[*pos[::-1]]\n",
    "        self._hsv_buffer[0]=middle_point\n",
    "        self._hsv_buffer=np.roll(self._hsv_buffer, 1, axis=0)\n",
    "        hsv_bounds = np.array([[0, 0, 0], [180, 255, 220]])  # Assuming Hue is in 0-180\n",
    "        mean=np.nanmean(self._hsv_buffer, axis=0 )\n",
    "        margin=[10, 50, 50]\n",
    "\n",
    "        # Calculate lower and upper bounds\n",
    "        lower_bound = np.clip(mean - margin, hsv_bounds[0], hsv_bounds[1])\n",
    "        upper_bound = np.clip(mean + margin, hsv_bounds[0], hsv_bounds[1])\n",
    "\n",
    "        # HSV margin array\n",
    "        self._obstacles_hsv = np.array([lower_bound, upper_bound])\n",
    "        np.save(self._cal_file, self._obstacles_hsv)\n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    vis=Vision(1, 4)\n",
    "    while True:\n",
    "        vis.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "\n",
    "The Vision class is responsible for 3 things:\n",
    "- Detection of the limits of the playground\n",
    "- Localisation of the Tymio and target\n",
    "- Localisation of the obstacles\n",
    "\n",
    "Some design choice were made early:\n",
    "- Use of aruco markes to detect the limits, Tymio and target\n",
    "- Use of a know rectangular playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection of the limits\n",
    "The first part of the update loop is to detect the aruco markers which are the limits of the playground (ID 0,1,6,3, repectively Top Left, TR, BR, BL). Once the limits are detected the image is \"warped\" in order to have rectangular shape. \n",
    "#### Aruco Detector\n",
    "To do this, we use the aruco detector class of the opencv python library  (which is an API for the c opencv library). The documentation can be found here https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html. The first step of the detection of an aruco detection is thresholding, the goal being to get a black and white image of the scene, with hopefully the black and white of the aruco each in a different color. The aruco detector class (of the C++ library) uses adaptive thresholding to reach this goal (for more details https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html), it requires transforming the image into a grayscale. Then it finds a threshold with the method \"ADAPTIVE_THRESH_MEAN_C\" (see https://github.com/opencv/opencv/blob/4.x/modules/objdetect/src/aruco/aruco_detector.cpp line 121). This defines the threshold for a position as the mean of square block around the position (the size of the square being defined by opencv) minux a constant C (all of the constant being optimised in opencv).\n",
    "\n",
    "The second step is to find the contours of the aruco tag. This is done with the suzuki algorithm. This works by first getting the edge points, probably using the canny algorithm. The canny algorithms works by getting the module of the gradient of the image. This is then fed into suzuki's algorithms which builds the actual contours. The contours are then simplified using the douglas-peucker algorithm (https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm#Algorithm nothing can explain it better that this gif). This gives a  polygon with 4 corners\n",
    "\n",
    "Once a credible polygon is found we can hop into last step, appyling a grid onto the found polygon. The number of white and black is counted in each grid cell and the bit coresponding to the square is the the one carried by the majority of pixels. This is then used to reorder the corners (in order to be able to use the orientation of the aruco)\n",
    "### Localisation of the Tymio and target\n",
    "Both Tymio and target have aruco tags (Tymio is 4 and target is 7). The position is found by the mean of the corners of the aruco detected on the warped image. The direction (for thymio) is found with the order of the aruco tag corners\n",
    "\n",
    "### Localisation of the obstacles\n",
    "\n",
    "The localisation of obstacles is done in 2 steps:\n",
    "- Thresholding, this is done by bounding thresholding the warped image in the HSV space. To have a precise HSV value, we have a calibration function and calibration aruco tag (which we can pause next to the color we want to threshold, its id is 5)\n",
    "- Finding contours, which done the same way as for aruco tags. It also passes through the DP algorithm because we want to limit the number of points to speed up the djikstra algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self, wheel_base=9.5):\n",
    "        # Wheel base of the robot\n",
    "        self.L = wheel_base\n",
    "\n",
    "        # Process noise covariance\n",
    "        q = 0.1 ** 2\n",
    "        self.Q = q * np.eye(3)\n",
    "\n",
    "        # Measurement noise covariance\n",
    "        self.R =np.diag([0.02**2,0.02**2, np.deg2rad(5)**2])\n",
    "\n",
    "        # Initial state estimate\n",
    "        self.x_hat = np.zeros(3)\n",
    "\n",
    "        # Initial covariance estimate\n",
    "        self.P = np.eye(3)\n",
    "\n",
    "        # Last time update was called\n",
    "        self.last_time = time.time()\n",
    "\n",
    "    def predict(self, u):\n",
    "        # Calculate the current time and dt\n",
    "        current_time = time.time()\n",
    "        dt = current_time - self.last_time\n",
    "        self.last_time = current_time\n",
    "\n",
    "        # Extract the state for readability\n",
    "        x, y, theta = self.x_hat\n",
    "\n",
    "        # Calculate the average speed\n",
    "        v = (u[0] + u[1]) / 2\n",
    "\n",
    "        # Update state\n",
    "        x += dt * v * np.cos(theta)\n",
    "        y += dt * v * np.sin(theta)\n",
    "        theta += dt * (u[0] - u[1]) / self.L\n",
    "\n",
    "        # Update state estimate\n",
    "        self.x_hat = np.array([x, y, theta])\n",
    "\n",
    "        # Jacobian of the motion model\n",
    "        F = np.array([[1, 0, -dt * v * np.sin(theta)],\n",
    "                      [0, 1, dt * v * np.cos(theta)],\n",
    "                      [0, 0, 1]])\n",
    "\n",
    "        # Predict covariance\n",
    "        self.P = np.dot(np.dot(F, self.P), F.T) + self.Q\n",
    "\n",
    "    def update(self, z, u):\n",
    "        # If the measurement is None, skip the update step\n",
    "        self.predict(u)\n",
    "        if z is None:\n",
    "            return\n",
    "\n",
    "        # Measurement model\n",
    "        H = np.eye(3)\n",
    "\n",
    "        # Kalman gain\n",
    "        K = np.dot(np.dot(self.P, H.T), np.linalg.inv(np.dot(np.dot(H, self.P), H.T) + self.R))\n",
    "\n",
    "        # Update state estimate\n",
    "        self.x_hat += np.dot(K, (z - self.x_hat))\n",
    "\n",
    "        # Update covariance estimate\n",
    "        self.P = self.P - np.dot(np.dot(K, H), self.P)\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kalman filter is responsible of keeping track of the robot's position when the camera feed breaks for whatever reason. To do this the Kalman filter uses 2 informations:\n",
    "- The wheel speeds\n",
    "- The Tymio position and orientation from the Vision part\n",
    "\n",
    "The Kalman filter works in 2 steps:\n",
    "- Predict step\n",
    "- Update step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict step\n",
    "This step determines where the robot is from the time passed since last prediction. Here the predicted position is  calculated by adding: $position = last\\_position + dt \\times speed$. The difference in angle comes from the difference in wheel speed divided by the wheel distance. The next state covariance (self.P) matrix is then calculated for the last one, the jacobian (F) and the process noise covariance (self.Q)\n",
    "\n",
    "#### Update step\n",
    "Here we compare the measurments we did with Vision with the predicted value. If we don't get any measurments from the camera, this part is skipped and self.P (state covariance) will keep growing until we get the camera back. If on the other hand we get the data from vision, we can calculate the Kalman gain matrix (K) which is then used to weight the measurment against the prediction (thus a prediction that has not been updated since a lot of time will have a very small weight against the camera)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "\n",
    "class PDController:\n",
    "    def __init__(self, kp=200, kd=0, default_speed=100, max_speed=200):\n",
    "        self.max_speed=max_speed\n",
    "        self.kp = kp  # Proportional gain\n",
    "        self.kd = kd  # Derivative gain\n",
    "        self.default_speed = default_speed\n",
    "        self.previous_error = 0\n",
    "        self.is_target_behind=False\n",
    "        self.follow_angle=False\n",
    "\n",
    "    def update(self, current_position, path):\n",
    "        if path is None: return (0,0)\n",
    "        self.follow_angle=(len(path)==1)\n",
    "        spd=self.default_speed\n",
    "        if self.follow_angle:\n",
    "            dist=0\n",
    "            control_signal = self._pd_control(current_position, path[0])\n",
    "            spd=50\n",
    "        else:\n",
    "            dist=np.linalg.norm(np.array(current_position[0:2])-np.array(path[-1]))\n",
    "            if dist<5:\n",
    "                return (0,0)\n",
    "            control_signal = self._pd_control(current_position, path[1])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if self.is_target_behind:\n",
    "            spd = 0  # \n",
    "        return self._calculate_wheel_speeds(control_signal, spd)\n",
    "    \n",
    "    def _calculate_error(self, current_position, target_position):\n",
    "        # Calculate the Euclidean distance as error\n",
    "        if not self.follow_angle:\n",
    "            current_position=np.array(current_position)\n",
    "            target_position=np.array(target_position)\n",
    "            vector=target_position-current_position[:2]\n",
    "            target_angle=mt.atan2(*vector[::-1])\n",
    "        else: \n",
    "            target_angle=target_position\n",
    "            print(\"PD c t\", current_position[2], target_angle)\n",
    "        angle_difference = target_angle-current_position[2]\n",
    "        angle_difference = (angle_difference + np.pi) % (2 * np.pi) - np.pi\n",
    "        self.is_target_behind = np.abs(angle_difference) > np.pi / 2\n",
    "        print(angle_difference)\n",
    "        return angle_difference\n",
    "\n",
    "    def _pd_control(self, current_position, target_position):\n",
    "        error = self._calculate_error(current_position, target_position)\n",
    "        derivative = error + self.previous_error\n",
    "        control_signal = self.kp * error - self.kd * derivative\n",
    "        self.previous_error = error\n",
    "        print(\"control_signal\", control_signal)\n",
    "        return control_signal\n",
    "    \n",
    "\n",
    "    def _calculate_wheel_speeds(self, control_signal, spd):\n",
    "        # Implement the logic to calculate wheel speeds based on your robot's design\n",
    "        # This is a placeholder for your specific implementation\n",
    "        left_wheel_speed = np.clip(spd + control_signal, -self.max_speed, self.max_speed)\n",
    "        right_wheel_speed = np.clip(spd - control_signal, -self.max_speed, self.max_speed)\n",
    "        return int(left_wheel_speed), int(right_wheel_speed)\n",
    "    def bound_value(value, min_value, max_value):\n",
    "        return max(min_value, min(value, max_value))\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    def motors(left, right):\n",
    "        return {\n",
    "            \"motor.left.target\": [left],\n",
    "            \"motor.right.target\": [right],\n",
    "        }\n",
    "    import vision\n",
    "    vis = vision.Vision(0,4)\n",
    "    pd  = PDController()\n",
    "    from tdmclient import ClientAsync, aw # ClientAsync.aw allows us to run asychronous functions sychronously\n",
    "    while True:\n",
    "        vis.update()\n",
    "        if vis.get_thymio_pos() is not None:\n",
    "            break\n",
    "    client = ClientAsync()                 # Create client object\n",
    "    client.process_waiting_messages()      # Ensure connection to TDM (Thymio Device Manager)\n",
    "    node = aw(client.wait_for_node()) # Ensure connection with Thymio robot (node)\n",
    "    aw(node.lock())     \n",
    "    while True:\n",
    "        vis.update()\n",
    "        pos=vis.get_thymio_pos()\n",
    "        if pos is None: continue\n",
    "        speeds=pd.update(pos, vis.get_target())\n",
    "        node.send_set_variables(motors(*speeds))\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The controller is a simple P controller (designed as PD) but since the kd was not necessary it was set as 0 (the robot has a very low inertia compared to the wheels torque).\n",
    "\n",
    "It has 2 working modes:\n",
    "- Follow point, this is used for global navigation, basically it calulates the angle to target and follows this angle\n",
    "- Follow angle, used in local navigation, it is used to follow the direction of the inverse of the gradient of the potential field.\n",
    "\n",
    "It is also responsible for stoping the robot one the final target is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
